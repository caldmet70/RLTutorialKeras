{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for A2C Reinforcement Learning using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MD Muhaimin Rahman\n",
    "sezan92[at]gmail[dot]com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have idea about Q Learning ,Deep Q Learning and Deep Neural Networks , then this tutorial is for you. Otherwise, you should learn them first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"libraries\"></a>\n",
    "### Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "import gym\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "SEED =123\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "steps_per_episode=200\n",
    "BATCH_SIZE=256\n",
    "TAU=0.001\n",
    "GAMMA=0.99\n",
    "actor_lr=0.001\n",
    "critic_lr=0.001\n",
    "SHOW= False\n",
    "action_list = [0,1]#,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"model\"></a>\n",
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some trials and errors, I have selected this network. The Actor Network is 3 layer MLP with 320 hidden nodes in each layer. The critic network is also a 3 layer MLP with 640 hidden nodes in each layer.Notice that the return arguments of function ```create_critic_network```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actor_network(state_shape,action_shape):\n",
    "    state=layers.Input(shape=state_shape,name=\"state\")\n",
    "    action_prob =layers.Input(shape=(1,),name=\"action_index\",dtype=\"int64\")\n",
    "    l1 =layers.Dense(320,activation=\"relu\")(state)\n",
    "    l2 =layers.Dense(320,activation=\"relu\")(l1)\n",
    "    l3 =layers.Dense(320,activation=\"relu\")(l2)\n",
    "    action =layers.Dense(action_shape,activation=\"softmax\")(l3)\n",
    "    actor= Model(state,action)\n",
    "    return actor,action_prob\n",
    "\n",
    "def create_critic_network(state_shape):\n",
    "    state = layers.Input(shape=state_shape,name=\"state\")\n",
    "    R_tensor = layers.Input(shape=(1,),name=\"R_tensor\")\n",
    "    l1 = layers.Dense(640,activation=\"relu\")(state)\n",
    "    l2 = layers.Dense(640,activation=\"relu\")(l1)\n",
    "    l3 = layers.Dense(640,activation=\"relu\")(l2)\n",
    "    value = layers.Dense(1)(l3)\n",
    "    critic = Model(inputs=state,outputs=value)\n",
    "    return critic,state,R_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am chosing ```MountainCar-v0``` game. Mainly because my GPU is not that good to work on higher dimensional state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape= env.observation_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape=(env.action_space.n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor,action_index = create_actor_network(state_shape,action_shape[0])\n",
    "critic,state_tensor,R_tensor = create_critic_network(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen ```RMSProp``` optimizer, due to more stability compared to Adam . I found it after trials and errors, no theoritical background on chosing this optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer = keras.optimizers.RMSprop(actor_lr)\n",
    "\n",
    "critic_optimizer = keras.optimizers.RMSprop(critic_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.compile(loss=\"mse\",optimizer=critic_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is the most critical part of ddpg in keras. The object ```critic``` and ```actor``` has a ```__call__``` method inside it, which will give output tensor if you give input a tensor. So to get the tensor object of ```Q``` we will use this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CriticValues = critic([state_tensor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage = R_tensor-CriticValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob=actor(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp=K.log(action_prob[0][action_index[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD=logp*advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy= -action_prob*logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_loss = -TD-0.01*entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.mean(action_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "updates = actor_optimizer.get_updates(params=actor.trainable_weights,loss=action_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function which will train the actor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_train = K.function(inputs=[state_tensor,R_tensor,action_index],outputs=[actor(state_tensor),\n",
    "                                                                      K.sum(action_loss)],\n",
    "                   updates=updates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"training\"></a>\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps_per_episodes=200\n",
    "max_total_reward=0\n",
    "for episode in range(num_episodes):\n",
    "    values =[]\n",
    "    action_probs=[]\n",
    "    rewards=[]\n",
    "    states=[]\n",
    "    action_probs=[]\n",
    "    terminals=[]\n",
    "    R_list=[]\n",
    "    advantages=[]\n",
    "    state= env.reset()\n",
    "    state = state.reshape((-1),)\n",
    "    total_reward=0\n",
    "    value_loss=0\n",
    "    action_loss=0\n",
    "#     states =deque(max)\n",
    "    for step in range(steps_per_episodes):\n",
    "        action_probability= actor.predict(state.reshape(1,-1))\n",
    "        action = np.random.choice(action_list,p=action_probability[0])\n",
    "        action_probability[action_probability!=action_probability[0][action]]=0\n",
    "        action_probability[action_probability==action_probability[0][action]]=1\n",
    "        action_probs.append(action)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        total_reward=total_reward+reward\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        terminals.append(done)\n",
    "        value = critic.predict(state.reshape(1,-1))\n",
    "        if SHOW:\n",
    "            env.render()\n",
    "        if done or step==(steps_per_episodes-1):\n",
    "            if total_reward<-199:\n",
    "                print(\"Failed!\",end=\" \")\n",
    "                R=0\n",
    "            elif total_reward>-199:\n",
    "                print(\"Passed!\",end=\" \")\n",
    "                R = value\n",
    "            break\n",
    "        \n",
    "        state=next_state\n",
    "        \n",
    "    print(\"Episode %d Total Reward %f\"%(episode,total_reward))\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        R = rewards[t]+GAMMA*R\n",
    "        R_list.append(R)\n",
    "        advantage = R-critic.predict(states[t].reshape(1,-1))\n",
    "        advantages.append(advantage)\n",
    "#         value_loss =value_loss+advantage**2\n",
    "#         action_prob = actor.predict(states[t].reshape(1,-1))\n",
    "#         action_log_prob = np.log(action_prob[0][action_probs[t]]+1e-5)\n",
    "#         entropy= -action_prob[0][action_probs[t]]*action_log_prob\n",
    "#         policy_loss = policy_loss-action_log_prob*advantage-0.01*entropy\n",
    "    states = np.vstack(states)\n",
    "    R_list= np.vstack(R_list)\n",
    "    action_probs = np.vstack(action_probs)\n",
    "    loss=critic.train_on_batch(x=states,y=R_list)\n",
    "    _,action_loss = actor_train(inputs=[states,R_list,action_probs])\n",
    "    print(\"action loss %f\"%action_loss)\n",
    "    #print(\"Weights \")\n",
    "    #print(actor.get_weights()[:-1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please watch at 2x speed. I changed some simple mistakes after the video so the rewards are not exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](http://img.youtube.com/vi/9Fe_n-ovIaA/0.jpg)](http://www.youtube.com/watch?v=9Fe_n-ovIaA \"Keras tutorial DDPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
