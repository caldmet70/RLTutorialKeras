first paragraph e jeikhane bola hoise mountain car problem ta discrete diye kora hoise but prb ta actually continuous and ei prb solve korar jonno ei new Algo ta ashse eikhane arektu explanation dorkar. Mane arektu elaboration je ashole prb ta ki hoy discrete diye.

Secondly learning table er sathe neural network er mapping er eikhane. Learning table er eikhane NN boshalam cause both have some similarities. kintu ei similarities ta ektu bolle valo hoy. even jara nn jane tader o ektu ei transition ta ektu sudden mone hbe. Ami onek khon dhore ei transition ta bujhar try korsi :'(. Neural er input 

Another thing kano model 3 bar run korbo ? ei jayga ta na bollei valo possibly. ektu confused kore dey. 

Next bellman equation .. NN er version derivative neoa hoise but power 2 er derivative. Why ? 

Another thing "It also helps randomizing the data , which in turn, helps to converge the model faster". Why randomization helps convergence ? 

Greedy Policy er oikhane Simulated annealing diye bujhaile purai kop hbe jinish ta. SOmething like this : We are playing a Mahjong tiles game. The moves we are doing after thinking a lot is GOOD moves( just like actions from DQN table ). But the moves we are doing whimsically are BAD moves( i.e. random actions ). It may seem that doing BAD moves are BAD for achieving GOOD scores. But in reality It's good to do some BAD moves sometimes. Sometimes doing BAD move at current step and GOOD move at next step increase the final score more than doing GOOD moves consecutively.It's statiscally proven :P 
